{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a03eb406-c203-4043-80c3-5f3745e05560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in d:\\application\\anaconda\\envs\\tf310\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in d:\\application\\anaconda\\envs\\tf310\\lib\\site-packages (from optuna) (1.16.3)\n",
      "Requirement already satisfied: colorlog in d:\\application\\anaconda\\envs\\tf310\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: numpy in d:\\application\\anaconda\\envs\\tf310\\lib\\site-packages (from optuna) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\application\\anaconda\\envs\\tf310\\lib\\site-packages (from optuna) (24.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in d:\\application\\anaconda\\envs\\tf310\\lib\\site-packages (from optuna) (2.0.41)\n",
      "Requirement already satisfied: tqdm in d:\\application\\anaconda\\envs\\tf310\\lib\\site-packages (from optuna) (4.67.1)\n",
      "Requirement already satisfied: PyYAML in d:\\application\\anaconda\\envs\\tf310\\lib\\site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: Mako in d:\\application\\anaconda\\envs\\tf310\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.12 in d:\\application\\anaconda\\envs\\tf310\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
      "Requirement already satisfied: tomli in d:\\application\\anaconda\\envs\\tf310\\lib\\site-packages (from alembic>=1.5.0->optuna) (2.0.1)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\application\\anaconda\\envs\\tf310\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
      "Requirement already satisfied: colorama in d:\\application\\anaconda\\envs\\tf310\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in d:\\application\\anaconda\\envs\\tf310\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "b501fd48-ab5b-4302-abc5-d3051a3a6d0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in d:\\application\\anaconda\\envs\\tf310\\lib\\site-packages (0.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "b57b9590-4307-4bb0-b5b3-b7bf8c924e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "06d9fddc-a1af-412f-ae12-255c679c71c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSVs\n",
    "ratings_df = pd.read_csv(\"./data/ratings.csv\")      # has userId, movieId, rating\n",
    "movies_df = pd.read_csv(\"./data/movies.csv\")        # has all movieId (and maybe title)\n",
    "\n",
    "# Get all unique users and movies\n",
    "all_users = ratings_df['userId'].unique()\n",
    "all_movies = movies_df['movieId'].unique()\n",
    "\n",
    "# Build full user–movie grid\n",
    "full_grid = pd.MultiIndex.from_product(\n",
    "    [all_users, all_movies],\n",
    "    names=[\"userId\", \"movieId\"]\n",
    ").to_frame(index=False)\n",
    "\n",
    "\n",
    "# Merge: left = full_grid, right = ratings(incl NA)\n",
    "mergedY = full_grid.merge(ratings_df[['userId', 'movieId', 'rating']], \n",
    "                         on=['userId', 'movieId'], how='left')\n",
    "\n",
    "# Fill missing ratings with 0\n",
    "mergedY['rating'] = mergedY['rating'].fillna(0)\n",
    "\n",
    "# Pivot to movie rows, user columns\n",
    "Y = mergedY.pivot(index='movieId', columns='userId', values='rating')\n",
    "\n",
    "# Remove row/column labels if you want raw matrix\n",
    "Y = Y.reset_index(drop=True)\n",
    "Y.columns.name = None\n",
    "\n",
    "# Save\n",
    "Y.to_csv(\"./data/small_movies_Y.csv\", index=False, header=False)\n",
    "\n",
    "\n",
    "\n",
    "# Add a column that flags actual interactions\n",
    "ratings_df['interaction'] = 1  # flag real interactions\n",
    "\n",
    "# Merge: left = full_grid, right = real interactions\n",
    "merged = full_grid.merge(ratings_df[['userId', 'movieId', 'interaction']], \n",
    "                         on=['userId', 'movieId'], how='left')\n",
    "\n",
    "# Fill missing interactions with 0\n",
    "merged['interaction'] = merged['interaction'].fillna(0).astype(int)\n",
    "\n",
    "# Pivot to movie rows, user columns\n",
    "R = merged.pivot(index='movieId', columns='userId', values='interaction')\n",
    "\n",
    "# Remove row/column labels if you want raw matrix\n",
    "R = R.reset_index(drop=True)\n",
    "R.columns.name = None\n",
    "\n",
    "# Save\n",
    "R.to_csv(\"./data/small_movies_R.csv\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "6dd13b89-4e4c-4545-904e-8c6d63bd6788",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_movies = Y.shape[0]\n",
    "num_users = Y.shape[1]\n",
    "num_features = 50  # number of latent features (you choose this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "86784d39-720a-47c4-b741-547ea5a167c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(num_movies, num_features)\n",
    "df_X = pd.DataFrame(X)\n",
    "df_X.to_csv(\"./data/small_movies_X.csv\", index=False, header=False)\n",
    "W = np.random.randn(num_users, num_features)\n",
    "df_W = pd.DataFrame(W)\n",
    "df_W.to_csv(\"./data/small_movies_W.csv\", index=False, header=False)\n",
    "b = np.zeros(num_users)  # or include item bias too\n",
    "df_b = pd.DataFrame(b)\n",
    "df_b.to_csv(\"./data/small_movies_b.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "b0601866-99f7-46db-8d77-963996e1496f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y (9742, 610) R (9742, 610)\n",
      "X (9742, 50)\n",
      "W (610, 50)\n",
      "b (610,)\n",
      "X (9742, 50)\n",
      "W (610, 50)\n",
      "b (610,)\n",
      "num_features 50\n",
      "num_movies 9742\n",
      "num_users 610\n",
      "Y (9742, 610) R (9742, 610)\n",
      "num_features 50\n",
      "num_movies 9742\n",
      "num_users 610\n"
     ]
    }
   ],
   "source": [
    "print(\"Y\", Y.shape, \"R\", R.shape)\n",
    "print(\"X\", X.shape)\n",
    "print(\"W\", W.shape)\n",
    "print(\"b\", b.shape)\n",
    "print(\"X\", X.shape)\n",
    "print(\"W\", W.shape)\n",
    "print(\"b\", b.shape)\n",
    "print(\"num_features\", num_features)\n",
    "print(\"num_movies\",   num_movies)\n",
    "print(\"num_users\",    num_users)\n",
    "print(\"Y\", Y.shape, \"R\", R.shape)\n",
    "print(\"num_features\", num_features)\n",
    "print(\"num_movies\",   num_movies)\n",
    "print(\"num_users\",    num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "b0c8cb26-ef45-407e-90fe-ce79c0c3ea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ratings_small():\n",
    "    file = open('./data/small_movies_Y.csv', 'rb')\n",
    "    Y = np.loadtxt(file,delimiter = \",\")\n",
    "\n",
    "    file = open('./data/small_movies_R.csv', 'rb')\n",
    "    R = np.loadtxt(file,delimiter = \",\")\n",
    "    return(Y,R)\n",
    "\n",
    "def load_precalc_para_small():\n",
    "\n",
    "    file = open('./data/small_movies_X.csv', 'rb')\n",
    "    X = np.loadtxt(file, delimiter = \",\")\n",
    "\n",
    "    file = open('./data/small_movies_W.csv', 'rb')\n",
    "    W = np.loadtxt(file,delimiter = \",\")\n",
    "\n",
    "    file = open('./data/small_movies_b.csv', 'rb')\n",
    "    b = np.loadtxt(file,delimiter = \",\")\n",
    "    b = b.reshape(1,-1)\n",
    "    num_movies, num_features = X.shape\n",
    "    num_users,_ = W.shape\n",
    "    return(X, W, b, num_movies, num_features, num_users)\n",
    "\n",
    "#Load data\n",
    "X, W, b, num_movies, num_features, num_users = load_precalc_para_small()\n",
    "Y, R = load_ratings_small()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "b42fd65a-91c4-46dd-b081-310afe29a5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 3146094.85\n",
      "Cost: 3146094.85\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\"\"\"\n",
    "Returns the cost for the content-based filtering\n",
    "Args:\n",
    "  X (ndarray (num_movies,num_features)): matrix of item features\n",
    "  W (ndarray (num_users,num_features)) : matrix of user parameters\n",
    "  b (ndarray (1, num_users)            : vector of user parameters\n",
    "  Y (ndarray (num_movies,num_users)    : matrix of user ratings of movies\n",
    "  R (ndarray (num_movies,num_users)    : matrix, where R(i, j) = 1 if the i-th movies was rated by the j-th user\n",
    "  lambda_ (float): regularization parameter\n",
    "Returns:\n",
    "  J (float) : Cost\n",
    "\"\"\"\n",
    "#Vectorized cost function implementation\n",
    "def cofi_cost_func(X, W, b, Y, R, lambda_):\n",
    "    j = (tf.linalg.matmul(X, tf.transpose(W)) + b - Y)*R\n",
    "    J = 0.5 * tf.reduce_sum(j**2) + (lambda_/2) * (tf.reduce_sum(X**2) + tf.reduce_sum(W**2))\n",
    "    return J\n",
    "\n",
    "J = cofi_cost_func(X, W, b, Y, R, 0)\n",
    "print(f\"Cost: {J:.2f}\")\n",
    "\n",
    "J = cofi_cost_func(X ,W ,b , Y ,R , 0);\n",
    "print(f\"Cost: {J:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "aa4a3504-7b92-41ea-ab19-16b1bec77862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating for movie 1 : 3.921 / 5\n"
     ]
    }
   ],
   "source": [
    "#  From the matrix, we can compute statistics like average rating.\n",
    "tsmean =  np.mean(Y[0, R[0, :].astype(bool)])\n",
    "print(f\"Average rating for movie 1 : {tsmean:0.3f} / 5\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "2c14587b-db92-4af0-b6fd-4cc2508b221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse_mae(Y_true, Y_pred, R):\n",
    "    # Only consider actual ratings (where R == 1)\n",
    "    error = (Y_pred - Y_true) * R\n",
    "    squared_error = np.square(error)\n",
    "    abs_error = np.abs(error)\n",
    "\n",
    "    mse = np.sum(squared_error) / np.sum(R)\n",
    "    mae = np.sum(abs_error) / np.sum(R)\n",
    "\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse, mae\n",
    "\n",
    "def normalizeRatings(Y, R):\n",
    "    \"\"\"\n",
    "    Preprocess data by subtracting mean rating for every movie (every row).\n",
    "    Only include real ratings R(i,j)=1.\n",
    "    [Ynorm, Ymean] = normalizeRatings(Y, R) normalized Y so that each movie\n",
    "    has a rating of 0 on average. Unrated moves then have a mean rating (0)\n",
    "    Returns the mean rating in Ymean.\n",
    "    \"\"\"\n",
    "    Ymean = (np.sum(Y*R,axis=1)/(np.sum(R, axis=1)+1e-12)).reshape(-1,1)\n",
    "    Ynorm = Y - np.multiply(Ymean, R) \n",
    "    return(Ynorm, Ymean)\n",
    "\n",
    "\n",
    "# Normalize the Dataset\n",
    "Ynorm, Ymean = normalizeRatings(Y, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "d802f17c-ce95-40d0-9268-63eb6ddce6ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1\n",
      "Epoch   0 | Train Loss: 6918362.0000 | Val Loss: 5694660.0000\n",
      "Epoch  50 | Train Loss: 2647242.2500 | Val Loss: 2507632.0000\n",
      "Epoch 100 | Train Loss: 1230522.8750 | Val Loss: 1172143.0000\n",
      "Epoch 150 | Train Loss: 613444.6250 | Val Loss: 581259.5625\n",
      "Epoch 200 | Train Loss: 322033.0312 | Val Loss: 302766.0938\n",
      "Epoch 250 | Train Loss: 179146.8906 | Val Loss: 166340.5312\n",
      "Epoch 300 | Train Loss: 107210.1484 | Val Loss: 97499.7500\n",
      "Epoch 350 | Train Loss: 70125.9375 | Val Loss: 61789.9688\n",
      "Epoch 400 | Train Loss: 50533.6523 | Val Loss: 42710.8984\n",
      "Epoch 450 | Train Loss: 39894.1094 | Val Loss: 32165.3281\n",
      "Train RMSE for Fold 1: 0.6800\n",
      "Train MAE  for Fold 1: 0.5135\n",
      "Test  RMSE for Fold 1: 0.8148\n",
      "Test  MAE  for Fold 1: 0.6150\n",
      "\n",
      "Fold 2\n",
      "Epoch   0 | Train Loss: 7163435.0000 | Val Loss: 5479692.0000\n",
      "Epoch  50 | Train Loss: 2702284.5000 | Val Loss: 2467635.5000\n",
      "Epoch 100 | Train Loss: 1255449.3750 | Val Loss: 1169016.1250\n",
      "Epoch 150 | Train Loss: 628748.0000 | Val Loss: 584921.6875\n",
      "Epoch 200 | Train Loss: 332454.2500 | Val Loss: 306413.9688\n",
      "Epoch 250 | Train Loss: 186580.1719 | Val Loss: 168733.3281\n",
      "Epoch 300 | Train Loss: 112734.1875 | Val Loss: 98739.0469\n",
      "Epoch 350 | Train Loss: 74427.8203 | Val Loss: 62201.2578\n",
      "Epoch 400 | Train Loss: 54051.2422 | Val Loss: 42571.9961\n",
      "Epoch 450 | Train Loss: 42901.2812 | Val Loss: 31668.3457\n",
      "Train RMSE for Fold 2: 0.6596\n",
      "Train MAE  for Fold 2: 0.4978\n",
      "Test  RMSE for Fold 2: 0.8879\n",
      "Test  MAE  for Fold 2: 0.6810\n",
      "\n",
      "Fold 3\n",
      "Epoch   0 | Train Loss: 7253862.0000 | Val Loss: 5462239.5000\n",
      "Epoch  50 | Train Loss: 2724168.2500 | Val Loss: 2477349.0000\n",
      "Epoch 100 | Train Loss: 1267602.7500 | Val Loss: 1179467.3750\n",
      "Epoch 150 | Train Loss: 636392.3750 | Val Loss: 592781.5000\n",
      "Epoch 200 | Train Loss: 337176.5312 | Val Loss: 311863.6250\n",
      "Epoch 250 | Train Loss: 189424.0625 | Val Loss: 172529.7812\n",
      "Epoch 300 | Train Loss: 114397.6641 | Val Loss: 101473.5781\n",
      "Epoch 350 | Train Loss: 75355.0703 | Val Loss: 64260.0664\n",
      "Epoch 400 | Train Loss: 54522.5391 | Val Loss: 44203.0703\n",
      "Epoch 450 | Train Loss: 43094.4844 | Val Loss: 33032.3164\n",
      "Train RMSE for Fold 3: 0.6479\n",
      "Train MAE  for Fold 3: 0.4882\n",
      "Test  RMSE for Fold 3: 0.9590\n",
      "Test  MAE  for Fold 3: 0.7315\n",
      "\n",
      "Fold 4\n",
      "Epoch   0 | Train Loss: 7205580.0000 | Val Loss: 5478775.0000\n",
      "Epoch  50 | Train Loss: 2712279.5000 | Val Loss: 2471271.7500\n",
      "Epoch 100 | Train Loss: 1260096.2500 | Val Loss: 1171802.2500\n",
      "Epoch 150 | Train Loss: 631455.2500 | Val Loss: 586967.7500\n",
      "Epoch 200 | Train Loss: 334294.6250 | Val Loss: 308061.5625\n",
      "Epoch 250 | Train Loss: 187967.3125 | Val Loss: 170153.7812\n",
      "Epoch 300 | Train Loss: 113806.6406 | Val Loss: 99965.4141\n",
      "Epoch 350 | Train Loss: 75234.5391 | Val Loss: 63228.2422\n",
      "Epoch 400 | Train Loss: 54628.8594 | Val Loss: 43404.9805\n",
      "Epoch 450 | Train Loss: 43291.9648 | Val Loss: 32329.8242\n",
      "Train RMSE for Fold 4: 0.6556\n",
      "Train MAE  for Fold 4: 0.4945\n",
      "Test  RMSE for Fold 4: 0.8739\n",
      "Test  MAE  for Fold 4: 0.6667\n",
      "\n",
      "Fold 5\n",
      "Epoch   0 | Train Loss: 7101914.5000 | Val Loss: 5533671.0000\n",
      "Epoch  50 | Train Loss: 2682693.0000 | Val Loss: 2475200.2500\n",
      "Epoch 100 | Train Loss: 1244277.2500 | Val Loss: 1167031.3750\n",
      "Epoch 150 | Train Loss: 621521.5625 | Val Loss: 582124.3750\n",
      "Epoch 200 | Train Loss: 327651.7812 | Val Loss: 304417.3438\n",
      "Epoch 250 | Train Loss: 183371.8906 | Val Loss: 167648.8594\n",
      "Epoch 300 | Train Loss: 110561.4219 | Val Loss: 98347.9766\n",
      "Epoch 350 | Train Loss: 72900.6094 | Val Loss: 62265.0547\n",
      "Epoch 400 | Train Loss: 52912.3906 | Val Loss: 42914.0312\n",
      "Epoch 450 | Train Loss: 41991.9727 | Val Loss: 32177.2227\n",
      "Train RMSE for Fold 5: 0.6646\n",
      "Train MAE  for Fold 5: 0.5012\n",
      "Test  RMSE for Fold 5: 0.8778\n",
      "Test  MAE  for Fold 5: 0.6723\n"
     ]
    }
   ],
   "source": [
    "# Normalize the Dataset\n",
    "Ynorm, Ymean = normalizeRatings(Y, R)\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Convert to numpy if not already\n",
    "Y = np.array(Ynorm)\n",
    "R = np.array(R)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "fold = 1\n",
    "for train_index, test_index in kf.split(Y.T):  # Y.T because users are columns\n",
    "    print(f\"\\nFold {fold}\")\n",
    "\n",
    "    # Build train/test masks\n",
    "    R_train = np.zeros_like(R)\n",
    "    R_test = np.zeros_like(R)\n",
    "    for idx in train_index:\n",
    "        R_train[:, idx] = R[:, idx]\n",
    "    for idx in test_index:\n",
    "        R_test[:, idx] = R[:, idx]\n",
    "\n",
    "    # Initialize model parameters\n",
    "    X = tf.Variable(np.random.randn(num_movies, num_features), dtype=tf.float32)\n",
    "    W = tf.Variable(np.random.randn(num_users, num_features), dtype=tf.float32)\n",
    "    b = tf.Variable(np.zeros((1, num_users)), dtype=tf.float32)\n",
    "    c = tf.Variable(np.zeros((num_movies, 1)), dtype=tf.float32)\n",
    "\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    lambda_ = 19.733122695917142\n",
    "\n",
    "    check_every = 50  # Print every N epochs\n",
    "    # Train on training set only\n",
    "    for epoch in range(500):\n",
    "        with tf.GradientTape() as tape:\n",
    "            cost = cofi_cost_func(X, W, b, Y, R_train, lambda_)\n",
    "        grads = tape.gradient(cost, [X, W, b])\n",
    "        optimizer.apply_gradients(zip(grads, [X, W, b]))\n",
    "        if epoch % check_every == 0:\n",
    "        # Compute validation loss\n",
    "            val_loss = cofi_cost_func(X, W, b, Y, R_test, lambda_)\n",
    "            print(f\"Epoch {epoch:3d} | Train Loss: {cost.numpy():.4f} | Val Loss: {val_loss.numpy():.4f}\")\n",
    "\n",
    "    # Final predictions in normalized space\n",
    "    preds = tf.matmul(X, tf.transpose(W)) + b\n",
    "    \n",
    "    # Convert preds back to original rating scale\n",
    "    preds_un = preds.numpy() + Ymean  # broadcasting (num_movies x 1)\n",
    "    \n",
    "    # Training metrics\n",
    "    Y_train = Y + Ymean  # unnormalize ground truth\n",
    "    train_diff = (preds_un - Y_train) * R_train\n",
    "    train_rmse = np.sqrt(np.sum(train_diff**2) / np.sum(R_train))\n",
    "    train_mae = np.sum(np.abs(train_diff)) / np.sum(R_train)\n",
    "    \n",
    "    # Validation metrics\n",
    "    test_diff = (preds_un - Y_train) * R_test\n",
    "    test_rmse = np.sqrt(np.sum(test_diff**2) / np.sum(R_test))\n",
    "    test_mae = np.sum(np.abs(test_diff)) / np.sum(R_test)\n",
    "\n",
    "\n",
    "    # Print both\n",
    "    print(f\"Train RMSE for Fold {fold}: {train_rmse:.4f}\")\n",
    "    print(f\"Train MAE  for Fold {fold}: {train_mae:.4f}\")\n",
    "    print(f\"Test  RMSE for Fold {fold}: {test_rmse:.4f}\")\n",
    "    print(f\"Test  MAE  for Fold {fold}: {test_mae:.4f}\")\n",
    "\n",
    "    fold += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "29b717d6-e70b-40bc-a3ec-0ac642e91812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>year</th>\n",
       "      <th>genre_Animation</th>\n",
       "      <th>genre_Comedy</th>\n",
       "      <th>genre_Adventure</th>\n",
       "      <th>genre_Musical</th>\n",
       "      <th>genre_Documentary</th>\n",
       "      <th>genre_Romance</th>\n",
       "      <th>genre_Crime</th>\n",
       "      <th>genre_(no genres listed)</th>\n",
       "      <th>...</th>\n",
       "      <th>genre_Action</th>\n",
       "      <th>genre_Children</th>\n",
       "      <th>genre_Thriller</th>\n",
       "      <th>genre_IMAX</th>\n",
       "      <th>genre_Fantasy</th>\n",
       "      <th>genre_War</th>\n",
       "      <th>genre_Film-Noir</th>\n",
       "      <th>genre_Mystery</th>\n",
       "      <th>genre_Drama</th>\n",
       "      <th>genre_Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>193581</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>193583</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>193585</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>193587</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>193609</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9742 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieId    year  genre_Animation  genre_Comedy  genre_Adventure  \\\n",
       "0           1  1995.0                1             1                1   \n",
       "1           2  1995.0                0             0                1   \n",
       "2           3  1995.0                0             1                0   \n",
       "3           4  1995.0                0             1                0   \n",
       "4           5  1995.0                0             1                0   \n",
       "...       ...     ...              ...           ...              ...   \n",
       "9737   193581  2017.0                1             1                0   \n",
       "9738   193583  2017.0                1             1                0   \n",
       "9739   193585  2017.0                0             0                0   \n",
       "9740   193587  2018.0                1             0                0   \n",
       "9741   193609  1991.0                0             1                0   \n",
       "\n",
       "      genre_Musical  genre_Documentary  genre_Romance  genre_Crime  \\\n",
       "0                 0                  0              0            0   \n",
       "1                 0                  0              0            0   \n",
       "2                 0                  0              1            0   \n",
       "3                 0                  0              1            0   \n",
       "4                 0                  0              0            0   \n",
       "...             ...                ...            ...          ...   \n",
       "9737              0                  0              0            0   \n",
       "9738              0                  0              0            0   \n",
       "9739              0                  0              0            0   \n",
       "9740              0                  0              0            0   \n",
       "9741              0                  0              0            0   \n",
       "\n",
       "      genre_(no genres listed)  ...  genre_Action  genre_Children  \\\n",
       "0                            0  ...             0               1   \n",
       "1                            0  ...             0               1   \n",
       "2                            0  ...             0               0   \n",
       "3                            0  ...             0               0   \n",
       "4                            0  ...             0               0   \n",
       "...                        ...  ...           ...             ...   \n",
       "9737                         0  ...             1               0   \n",
       "9738                         0  ...             0               0   \n",
       "9739                         0  ...             0               0   \n",
       "9740                         0  ...             1               0   \n",
       "9741                         0  ...             0               0   \n",
       "\n",
       "      genre_Thriller  genre_IMAX  genre_Fantasy  genre_War  genre_Film-Noir  \\\n",
       "0                  0           0              1          0                0   \n",
       "1                  0           0              1          0                0   \n",
       "2                  0           0              0          0                0   \n",
       "3                  0           0              0          0                0   \n",
       "4                  0           0              0          0                0   \n",
       "...              ...         ...            ...        ...              ...   \n",
       "9737               0           0              1          0                0   \n",
       "9738               0           0              1          0                0   \n",
       "9739               0           0              0          0                0   \n",
       "9740               0           0              0          0                0   \n",
       "9741               0           0              0          0                0   \n",
       "\n",
       "      genre_Mystery  genre_Drama  genre_Western  \n",
       "0                 0            0              0  \n",
       "1                 0            0              0  \n",
       "2                 0            0              0  \n",
       "3                 0            1              0  \n",
       "4                 0            0              0  \n",
       "...             ...          ...            ...  \n",
       "9737              0            0              0  \n",
       "9738              0            0              0  \n",
       "9739              0            1              0  \n",
       "9740              0            0              0  \n",
       "9741              0            0              0  \n",
       "\n",
       "[9742 rows x 22 columns]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "movies_df = pd.read_csv('./data/movies.csv')\n",
    "\n",
    "# --- Extract year from title ---\n",
    "def extract_year(title):\n",
    "    match = re.search(r\"\\((\\d{4})\\)\", title)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "movies_df['year'] = movies_df['title'].apply(extract_year)\n",
    "\n",
    "# --- One-hot encode genres ---\n",
    "genre_cols = list(set('|'.join(movies_df['genres']).split('|')))\n",
    "for genre in genre_cols:\n",
    "    movies_df[f'genre_{genre}'] = movies_df['genres'].apply(lambda x: int(genre in x))\n",
    "\n",
    "# Drop unneeded columns\n",
    "cb_features = movies_df.drop(columns=['title', 'genres'])\n",
    "cb_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "194c1077-b2ef-4894-90a5-f45932980b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>genre_Animation</th>\n",
       "      <th>genre_Comedy</th>\n",
       "      <th>genre_Adventure</th>\n",
       "      <th>genre_Musical</th>\n",
       "      <th>genre_Documentary</th>\n",
       "      <th>genre_Romance</th>\n",
       "      <th>genre_Crime</th>\n",
       "      <th>genre_(no genres listed)</th>\n",
       "      <th>genre_Sci-Fi</th>\n",
       "      <th>...</th>\n",
       "      <th>genre_Action</th>\n",
       "      <th>genre_Children</th>\n",
       "      <th>genre_Thriller</th>\n",
       "      <th>genre_IMAX</th>\n",
       "      <th>genre_Fantasy</th>\n",
       "      <th>genre_War</th>\n",
       "      <th>genre_Film-Noir</th>\n",
       "      <th>genre_Mystery</th>\n",
       "      <th>genre_Drama</th>\n",
       "      <th>genre_Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1995.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1995.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1995.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1995.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1995.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          year  genre_Animation  genre_Comedy  genre_Adventure  genre_Musical  \\\n",
       "0       1995.0                1             1                1              0   \n",
       "1       1995.0                0             1                0              0   \n",
       "2       1995.0                0             0                0              0   \n",
       "3       1995.0                0             0                0              0   \n",
       "4       1995.0                0             0                0              0   \n",
       "...        ...              ...           ...              ...            ...   \n",
       "100831  2017.0                0             0                0              0   \n",
       "100832  2017.0                0             0                0              0   \n",
       "100833  2017.0                0             0                0              0   \n",
       "100834  2017.0                0             0                0              0   \n",
       "100835  2017.0                0             0                0              0   \n",
       "\n",
       "        genre_Documentary  genre_Romance  genre_Crime  \\\n",
       "0                       0              0            0   \n",
       "1                       0              1            0   \n",
       "2                       0              0            1   \n",
       "3                       0              0            0   \n",
       "4                       0              0            1   \n",
       "...                   ...            ...          ...   \n",
       "100831                  0              0            0   \n",
       "100832                  0              0            1   \n",
       "100833                  0              0            0   \n",
       "100834                  0              0            0   \n",
       "100835                  0              0            1   \n",
       "\n",
       "        genre_(no genres listed)  genre_Sci-Fi  ...  genre_Action  \\\n",
       "0                              0             0  ...             0   \n",
       "1                              0             0  ...             0   \n",
       "2                              0             0  ...             1   \n",
       "3                              0             0  ...             0   \n",
       "4                              0             0  ...             0   \n",
       "...                          ...           ...  ...           ...   \n",
       "100831                         0             0  ...             0   \n",
       "100832                         0             0  ...             1   \n",
       "100833                         0             0  ...             0   \n",
       "100834                         0             1  ...             1   \n",
       "100835                         0             0  ...             1   \n",
       "\n",
       "        genre_Children  genre_Thriller  genre_IMAX  genre_Fantasy  genre_War  \\\n",
       "0                    1               0           0              1          0   \n",
       "1                    0               0           0              0          0   \n",
       "2                    0               1           0              0          0   \n",
       "3                    0               1           0              0          0   \n",
       "4                    0               1           0              0          0   \n",
       "...                ...             ...         ...            ...        ...   \n",
       "100831               0               1           0              0          0   \n",
       "100832               0               1           0              0          0   \n",
       "100833               0               0           0              0          0   \n",
       "100834               0               0           0              0          0   \n",
       "100835               0               1           0              0          0   \n",
       "\n",
       "        genre_Film-Noir  genre_Mystery  genre_Drama  genre_Western  \n",
       "0                     0              0            0              0  \n",
       "1                     0              0            0              0  \n",
       "2                     0              0            0              0  \n",
       "3                     0              1            0              0  \n",
       "4                     0              1            0              0  \n",
       "...                 ...            ...          ...            ...  \n",
       "100831                0              0            1              0  \n",
       "100832                0              0            0              0  \n",
       "100833                0              0            0              0  \n",
       "100834                0              0            0              0  \n",
       "100835                0              0            1              0  \n",
       "\n",
       "[100836 rows x 21 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_df = pd.read_csv('./data/ratings.csv')\n",
    "\n",
    "cb_dataset = ratings_df.merge(cb_features, on='movieId')\n",
    "\n",
    "# Features: genre & year\n",
    "X_cb = cb_dataset.drop(columns=['userId', 'movieId', 'rating', 'timestamp'])\n",
    "y_cb = cb_dataset['rating']\n",
    "X_cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "0cb6aa36-14d8-4fc2-a010-1d8fca3fa7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_preds_full = preds_un  # shape: (num_movies, num_users)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "29c68a6b-3340-4f1c-8a76-bf9b9c722f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSVs\n",
    "ratings = pd.read_csv('./data/ratings.csv')\n",
    "movies = pd.read_csv('./data/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "0dd09e2a-41bb-41ae-8549-971572f473d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Files created: content_user_train.csv and content_item_train.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load data\n",
    "ratings = pd.read_csv('./data/ratings.csv')\n",
    "movies = pd.read_csv('./data/movies.csv')\n",
    "\n",
    "# ---------------------------------------\n",
    "# 🎬 Create content_movie.csv\n",
    "# ---------------------------------------\n",
    "\n",
    "# Extract year and encode genres (same as before)\n",
    "movies['year'] = movies['title'].apply(extract_year)\n",
    "movies['genres'] = movies['genres'].replace('(no genres listed)', '')\n",
    "genre_ohe = movies['genres'].str.get_dummies(sep='|')\n",
    "\n",
    "# Combine movie features\n",
    "movie_features = pd.concat([movies[['movieId', 'year']], genre_ohe], axis=1)\n",
    "\n",
    "# Add average rating per movie\n",
    "movie_rating_ave = ratings.groupby('movieId')['rating'].mean().reset_index(name='ave_rating')\n",
    "movie_features = pd.merge(movie_features, movie_rating_ave, on='movieId', how='left')\n",
    "\n",
    "# 🔥 KEY STEP: Merge with ratings to get one row per rating\n",
    "content_item_train = pd.merge(ratings[['userId', 'movieId', 'rating']], \n",
    "                             movie_features, \n",
    "                             on='movieId', \n",
    "                             how='left')\n",
    "\n",
    "# Drop userId and rating columns before saving features\n",
    "content_item_features = content_item_train.drop(['userId', 'rating'], axis=1)\n",
    "content_item_features.to_csv('./data/content_item_train.csv', index=False, header=False)\n",
    "\n",
    "# Save the ratings separately\n",
    "ratings['rating'].to_csv('./data/content_y_train.csv', index=False, header=False)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "# 👤 Create content_user_train.csv\n",
    "# ---------------------------------------\n",
    "\n",
    "# Merge ratings with movies to get genres\n",
    "ratings_movies = pd.merge(ratings, movies[['movieId', 'genres']], on='movieId')\n",
    "\n",
    "# Explode genres\n",
    "ratings_movies['genres'] = ratings_movies['genres'].str.split('|')\n",
    "ratings_genres = ratings_movies.explode('genres')\n",
    "\n",
    "# Drop empty genre values (in case of '(no genres listed)')\n",
    "ratings_genres = ratings_genres[ratings_genres['genres'] != '']\n",
    "\n",
    "# Per-user rating count and average\n",
    "user_stats = ratings.groupby('userId')['rating'].agg(\n",
    "    rating_count='count',\n",
    "    rating_ave='mean'\n",
    ").reset_index()\n",
    "\n",
    "# Per-user, per-genre average rating\n",
    "genre_stats = ratings_genres.groupby(['userId', 'genres'])['rating'].mean().unstack(fill_value=0)\n",
    "genre_stats.columns = [f\"{genre}_ave\" for genre in genre_stats.columns]\n",
    "genre_stats = genre_stats.reset_index()\n",
    "\n",
    "# Combine\n",
    "content_user_train = pd.merge(user_stats, genre_stats, on='userId')\n",
    "\n",
    "# Save to CSV\n",
    "content_user_train.to_csv('./data/content_user_train.csv', index=False, header=False)\n",
    "\n",
    "print(\"✅ Files created: content_user_train.csv and content_item_train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "1c986028-98f9-495c-9c6e-4974822c8780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ content_items_vecs.csv created with only rated movies.\n"
     ]
    }
   ],
   "source": [
    "ratings = pd.read_csv('./data/ratings.csv')\n",
    "ratings['rating'].to_csv('./data/content_y_train.csv', index=False, header=False)\n",
    "\n",
    "\n",
    "# # Keep only movies with ratings\n",
    "rated_movie_ids = ratings['movieId'].unique()\n",
    "rated_movies = movies[movies['movieId'].isin(rated_movie_ids)]\n",
    "\n",
    "# Format output: movieId<TAB>title<TAB>genres\n",
    "rated_movies[['movieId', 'title', 'genres']].to_csv(\"./data/content_movie_list.csv\",index=False, header=False)# ---- Step 1: Extract year from movie title ----\n",
    "\n",
    "def extract_year(title):\n",
    "    match = re.search(r'\\((\\d{4})\\)$', title)\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "movies['year'] = movies['title'].apply(extract_year)\n",
    "\n",
    "# ---- Step 2: Compute average rating per movie ----\n",
    "movie_avg_rating = ratings.groupby('movieId')['rating'].mean().reset_index(name='ave_rating')\n",
    "\n",
    "# ---- Step 3: One-hot encode genres ----\n",
    "movies['genres'] = movies['genres'].replace('(no genres listed)', '')\n",
    "genre_ohe = movies['genres'].str.get_dummies(sep='|')\n",
    "\n",
    "# ---- Step 4: Combine features ----\n",
    "movies_with_genres = pd.concat([movies[['movieId', 'year']], genre_ohe], axis=1)\n",
    "\n",
    "# ✅ Only include movies that have ratings\n",
    "content_items_vecs = pd.merge(movie_avg_rating, movies_with_genres, on='movieId', how='inner')\n",
    "\n",
    "# ---- Step 5: Save to CSV ----\n",
    "content_items_vecs.to_csv('./data/content_items_vecs.csv', index=False, header=False)\n",
    "\n",
    "print(\"✅ content_items_vecs.csv created with only rated movies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "a55f38b8-10b6-41fb-bd9c-ef422590516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "# Load MovieLens data\n",
    "ratings = pd.read_csv('./data/ratings.csv')     # Contains: userId, movieId, rating, timestamp\n",
    "movies = pd.read_csv('./data/movies.csv')       # Contains: movieId, title, genres\n",
    "\n",
    "# Define the full genre list including the \"(no genres listed)\" category\n",
    "genre_list = [\n",
    "    'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy', 'Crime',\n",
    "    'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical',\n",
    "    'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western', '(no genres listed)'\n",
    "]\n",
    "\n",
    "# Create a genre vector from a genre string\n",
    "def get_genre_vector(genres):\n",
    "    genre_vector = np.zeros(len(genre_list))\n",
    "    if genres == '(no genres listed)':\n",
    "        genre_vector[-1] = 1  # Set the last index for \"no genres\"\n",
    "    else:\n",
    "        for i, genre in enumerate(genre_list):\n",
    "            if genre in genres:\n",
    "                genre_vector[i] = 1\n",
    "    return genre_vector\n",
    "\n",
    "# Create a map from movieId to genre vector\n",
    "movie_genre_map = {\n",
    "    row['movieId']: get_genre_vector(row['genres'].split('|'))\n",
    "    for _, row in movies.iterrows()\n",
    "}\n",
    "\n",
    "# Initialize the result dictionary\n",
    "user_to_genre = defaultdict(lambda: {\n",
    "    'glist': np.zeros((1, len(genre_list))),\n",
    "    'g_count': np.zeros((1, len(genre_list))),\n",
    "    'rating_count': 0,\n",
    "    'rating_sum': 0.0,\n",
    "    'movies': {},\n",
    "    'rating_ave': 0.0\n",
    "})\n",
    "\n",
    "# Process each rating\n",
    "for _, row in ratings.iterrows():\n",
    "    uid = row['userId']\n",
    "    mid = row['movieId']\n",
    "    rating = row['rating']\n",
    "\n",
    "    if mid not in movie_genre_map:\n",
    "        continue  # Skip if the movie is not in the genre map\n",
    "\n",
    "    genre_vec = movie_genre_map[mid]\n",
    "\n",
    "    user_data = user_to_genre[uid]\n",
    "    user_data['glist'] += genre_vec * rating\n",
    "    user_data['g_count'] += genre_vec\n",
    "    user_data['rating_count'] += 1\n",
    "    user_data['rating_sum'] += rating\n",
    "    user_data['movies'][mid] = rating\n",
    "\n",
    "# Finalize average rating\n",
    "for uid, data in user_to_genre.items():\n",
    "    if data['rating_count'] > 0:\n",
    "        data['rating_ave'] = round(data['rating_sum'] / data['rating_count'], 2)\n",
    "\n",
    "# Save the result as a pickle file\n",
    "with open('./data/user_to_genre.pickle', 'wb') as f:\n",
    "    pickle.dump(dict(user_to_genre), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "fb1e4b9c-9fe5-43ad-8004-c16916900193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    ''' called to load preprepared data for the lab '''\n",
    "    df = pd.read_csv('./data/content_item_train.csv')\n",
    "    print(df.isnull().sum())  # See which columns have NaNs\n",
    "    item_train = df.select_dtypes(include=[np.number]).values\n",
    "\n",
    "    # Load user features (without userId)\n",
    "    user_features_array = pd.read_csv('./data/content_user_train.csv', header=None).values\n",
    "    \n",
    "    # Load ratings\n",
    "    ratings = pd.read_csv('./data/ratings.csv')\n",
    "    \n",
    "    # Get unique users in the same order as your user features\n",
    "    unique_users = sorted(ratings['userId'].unique())\n",
    "    \n",
    "    # Create user features DataFrame with userId\n",
    "    user_features_df = pd.DataFrame(user_features_array)\n",
    "    user_features_df['userId'] = unique_users\n",
    "    \n",
    "    # Merge to get user features for each rating\n",
    "    ratings_with_user_features = pd.merge(\n",
    "        ratings[['userId', 'movieId', 'rating']], \n",
    "        user_features_df, \n",
    "        on='userId', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Extract user features (excluding userId, movieId, rating)\n",
    "    user_train = ratings_with_user_features.drop(['userId', 'movieId', 'rating'], axis=1).values\n",
    "\n",
    "    \n",
    "    y_train    = genfromtxt('./data/content_y_train.csv', delimiter=',')\n",
    "    with open('./data/content_item_train_header.txt', newline='') as f:    #csv reader handles quoted strings better\n",
    "        item_features = list(csv.reader(f))[0]\n",
    "    with open('./data/content_user_train_header.txt', newline='') as f:\n",
    "        user_features = list(csv.reader(f))[0]\n",
    "    item_vecs = genfromtxt('./data/content_item_vecs.csv', delimiter=',')\n",
    "\n",
    "    movie_dict = defaultdict(dict)\n",
    "    count = 0\n",
    "#    with open('./data/movies.csv', newline='') as csvfile:\n",
    "    with open('./data/content_movie_list.csv', newline='',encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for line in reader:\n",
    "            if count == 0:\n",
    "                count += 1  #skip header\n",
    "                #print(line) print\n",
    "            else:\n",
    "                count += 1\n",
    "                movie_id = int(line[0])\n",
    "                movie_dict[movie_id][\"title\"] = line[1]\n",
    "                movie_dict[movie_id][\"genres\"] = line[2]\n",
    "\n",
    "    with open('./data/content_user_to_genre.pickle', 'rb') as f:\n",
    "        user_to_genre = pickle.load(f)\n",
    "\n",
    "    return(item_train, user_train, y_train, item_features, user_features, item_vecs, movie_dict, user_to_genre)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "727153cd-0e1e-4b13-8b64-e3ed4f92d8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                      0\n",
      "1995.0                18\n",
      "0                      0\n",
      "1.1                    0\n",
      "1.2                    0\n",
      "1.3                    0\n",
      "1.4                    0\n",
      "0.1                    0\n",
      "0.2                    0\n",
      "0.3                    0\n",
      "1.5                    0\n",
      "0.4                    0\n",
      "0.5                    0\n",
      "0.6                    0\n",
      "0.7                    0\n",
      "0.8                    0\n",
      "0.9                    0\n",
      "0.10                   0\n",
      "0.11                   0\n",
      "0.12                   0\n",
      "0.13                   0\n",
      "3.9209302325581397     0\n",
      "dtype: int64\n",
      "user_train.shape: (100836, 22)\n",
      "Number of training vectors: 100835\n",
      "              0       1    2    3    4    5    6    7    8    9   ...   12  \\\n",
      "0            3.0  1995.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  0.0   \n",
      "1            6.0  1995.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0   \n",
      "2           47.0  1995.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
      "3           50.0  1995.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0   \n",
      "4           70.0  1996.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...  1.0   \n",
      "...          ...     ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "100830  166534.0  2017.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  ...  1.0   \n",
      "100831  168248.0  2017.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  ...  0.0   \n",
      "100832  168250.0  2017.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  1.0   \n",
      "100833  168252.0  2017.0  1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
      "100834  170875.0  2017.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  1.0  ...  0.0   \n",
      "\n",
      "         13   14   15   16   17   18   19   20        21  \n",
      "0       0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  3.259615  \n",
      "1       0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  3.946078  \n",
      "2       0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  3.975369  \n",
      "3       0.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  4.237745  \n",
      "4       0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  3.509091  \n",
      "...     ...  ...  ...  ...  ...  ...  ...  ...       ...  \n",
      "100830  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  3.333333  \n",
      "100831  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  4.142857  \n",
      "100832  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  3.633333  \n",
      "100833  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  4.280000  \n",
      "100834  0.0  0.0  0.0  0.0  0.0  1.0  0.0  0.0  2.333333  \n",
      "\n",
      "[100835 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "import csv\n",
    "\n",
    "\n",
    "# Load Data, set configuration variables\n",
    "item_train, user_train, y_train, item_features, user_features, item_vecs, movie_dict, user_to_genre = load_data()\n",
    "print(\"user_train.shape:\", user_train.shape)\n",
    "\n",
    "num_user_features = int(user_train.shape[1] - 3)  # remove userid, rating count and ave rating during training\n",
    "num_item_features = item_train.shape[1] - 1  # remove movie id at train time\n",
    "uvs = 3  # user genre vector start\n",
    "ivs = 3  # item genre vector start\n",
    "u_s = 3  # start of columns to use in training, user\n",
    "i_s = 1  # start of columns to use in training, items\n",
    "print(f\"Number of training vectors: {len(item_train)}\")\n",
    "print(pd.DataFrame(item_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "0df3a516-34e1-4b4a-bfed-460fb8568dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train[:5]: [4. 4. 4. 5. 5.]\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_train[:5]: {y_train[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "9a9ad866-e364-40db-bc38-19ac8bae57c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in item_train: 18\n",
      "NaNs in user_train: 0\n",
      "0\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scale training data\n",
    "item_train_unscaled = item_train\n",
    "user_train_unscaled = user_train\n",
    "y_train_unscaled    = y_train\n",
    "\n",
    "print(f\"NaNs in item_train: {np.isnan(item_train).sum()}\")\n",
    "print(f\"NaNs in user_train: {np.isnan(user_train).sum()}\")\n",
    "\n",
    "# Fix NaNs\n",
    "item_train = np.nan_to_num(item_train, nan=0.0)\n",
    "user_train = np.nan_to_num(user_train, nan=0.0)  # Just in case\n",
    "\n",
    "# UPDATE: Also update the reference data\n",
    "item_train_unscaled = np.nan_to_num(item_train_unscaled, nan=0.0)\n",
    "user_train_unscaled = np.nan_to_num(user_train_unscaled, nan=0.0)\n",
    "\n",
    "scalerItem = StandardScaler()\n",
    "print(np.isnan(item_train).sum())\n",
    "\n",
    "scalerItem.fit(item_train)\n",
    "item_train = scalerItem.transform(item_train)\n",
    "\n",
    "scalerUser = StandardScaler()\n",
    "scalerUser.fit(user_train)\n",
    "user_train = scalerUser.transform(user_train)\n",
    "\n",
    "scalerTarget = MinMaxScaler((-1, 1))\n",
    "scalerTarget.fit(y_train.reshape(-1, 1))\n",
    "y_train = scalerTarget.transform(y_train.reshape(-1, 1))\n",
    "#ynorm_test = scalerTarget.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "print(np.allclose(item_train_unscaled, scalerItem.inverse_transform(item_train)))\n",
    "print(np.allclose(user_train_unscaled, scalerUser.inverse_transform(user_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "d644fb3c-0003-41b1-84ae-dad641904c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie/item training data shape: (80668, 22)\n",
      "movie/item test data shape: (20167, 22)\n"
     ]
    }
   ],
   "source": [
    "item_train, item_test = train_test_split(item_train, train_size=0.80, shuffle=True, random_state=1)\n",
    "user_train, user_test = train_test_split(user_train, train_size=0.80, shuffle=True, random_state=1)\n",
    "y_train, y_test       = train_test_split(y_train,    train_size=0.80, shuffle=True, random_state=1)\n",
    "print(f\"movie/item training data shape: {item_train.shape}\")\n",
    "print(f\"movie/item test data shape: {item_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a43b7324-89e3-426d-86c3-bcd20bc14ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_18\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_18\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ sequential_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │          <span style=\"color: #00af00; text-decoration-color: #00af00\">42,144</span> │ input_layer_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ sequential_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │          <span style=\"color: #00af00; text-decoration-color: #00af00\">42,656</span> │ input_layer_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lambda_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lambda_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dot_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lambda_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],           │\n",
       "│                               │                           │                 │ lambda_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_33 (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ input_layer_35 (\u001b[38;5;33mInputLayer\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ sequential_32 (\u001b[38;5;33mSequential\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │          \u001b[38;5;34m42,144\u001b[0m │ input_layer_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ sequential_33 (\u001b[38;5;33mSequential\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │          \u001b[38;5;34m42,656\u001b[0m │ input_layer_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lambda_11 (\u001b[38;5;33mLambda\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ sequential_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lambda_12 (\u001b[38;5;33mLambda\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ sequential_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dot_4 (\u001b[38;5;33mDot\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │ lambda_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],           │\n",
       "│                               │                           │                 │ lambda_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">84,800</span> (331.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m84,800\u001b[0m (331.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">84,800</span> (331.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m84,800\u001b[0m (331.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GRADED_CELL\n",
    "# UNQ_C1\n",
    "\n",
    "num_outputs = 32\n",
    "tf.random.set_seed(1)\n",
    "user_NN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs)\n",
    "])\n",
    "\n",
    "item_NN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs) \n",
    "])\n",
    "\n",
    "# create the user input and point to the base network\n",
    "input_user = tf.keras.layers.Input(shape=(num_user_features,))\n",
    "vu = user_NN(input_user)\n",
    "vu = tf.keras.layers.Lambda(lambda x: tf.linalg.l2_normalize(x, axis=1))(vu)\n",
    "\n",
    "# create the item input and point to the base network\n",
    "input_item = tf.keras.layers.Input(shape=(num_item_features,))\n",
    "vm = item_NN(input_item)\n",
    "vm = tf.keras.layers.Lambda(lambda x: tf.linalg.l2_normalize(x, axis=1))(vm)\n",
    "\n",
    "# compute the dot product of the two vectors vu and vm\n",
    "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
    "\n",
    "# specify the inputs and output of the model\n",
    "model = tf.keras.Model([input_user, input_item], output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "eaa545d5-76e6-4046-8438-81442c922d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "cost_fn = tf.keras.losses.MeanSquaredError()\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt,\n",
    "              loss=cost_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "9cc98755-c104-4e2e-9577-ff7224367f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split all arrays together\n",
    "arrays_to_split = [item_train, user_train, y_train]\n",
    "split_arrays = train_test_split(*arrays_to_split, train_size=0.80, shuffle=True, random_state=1)\n",
    "\n",
    "item_train, item_test, user_train, user_test, y_train, y_test = split_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "7e209e29-9bd6-4ff9-a581-da050b6ed43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1392\n",
      "Epoch 2/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.1391\n",
      "Epoch 3/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.1390\n",
      "Epoch 4/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.1388\n",
      "Epoch 5/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.1386\n",
      "Epoch 6/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1385\n",
      "Epoch 7/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.1384\n",
      "Epoch 8/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.1383\n",
      "Epoch 9/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.1381\n",
      "Epoch 10/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1380\n",
      "Epoch 11/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1378\n",
      "Epoch 12/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.1377\n",
      "Epoch 13/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.1376\n",
      "Epoch 14/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.1375\n",
      "Epoch 15/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1373\n",
      "Epoch 16/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.1372\n",
      "Epoch 17/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.1371\n",
      "Epoch 18/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1369\n",
      "Epoch 19/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.1368\n",
      "Epoch 20/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.1366\n",
      "Epoch 21/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.1365\n",
      "Epoch 22/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.1363\n",
      "Epoch 23/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.1363\n",
      "Epoch 24/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 0.1361\n",
      "Epoch 25/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.1359\n",
      "Epoch 26/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - loss: 0.1359\n",
      "Epoch 27/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 0.1358\n",
      "Epoch 28/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.1356\n",
      "Epoch 29/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1355\n",
      "Epoch 30/30\n",
      "\u001b[1m2017/2017\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 0.1353\n",
      "\u001b[1m505/505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step  \n",
      "Test RMSE: 0.4441\n",
      "\u001b[1m505/505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.1977    \n",
      "Test MSE from model.evaluate: 0.1972\n",
      "Test RMSE: 0.4441\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "tf.random.set_seed(1)\n",
    "model.fit([user_train[:, u_s:], item_train[:, i_s:]], y_train, epochs=30)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = model.predict([user_test[:, u_s:], item_test[:, i_s:]])\n",
    "\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred) ** 2))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "loss = model.evaluate([user_test[:, u_s:], item_test[:, i_s:]], y_test)\n",
    "print(f\"Test MSE from model.evaluate: {loss:.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(loss):.4f}\")  # Should match manual RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "1a02b4a0-7132-4f81-82a1-16a8b7c38fa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m505/505\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 958us/step - loss: 0.1977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19721217453479767"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([user_test[:, u_s:], item_test[:, i_s:]], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "2d39bc5e-dae2-4864-8286-d0992b8f91ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9725"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_user_id = 5000\n",
    "new_rating_ave = 0.0\n",
    "new_action = 0.0\n",
    "new_adventure = 5.0\n",
    "new_animation = 0.0\n",
    "new_childrens = 0.0\n",
    "new_comedy = 0.0\n",
    "new_crime = 0.0\n",
    "new_documentary = 0.0\n",
    "new_drama = 0.0\n",
    "new_fantasy = 5.0\n",
    "new_horror = 0.0\n",
    "new_mystery = 0.0\n",
    "new_romance = 0.0\n",
    "new_scifi = 0.0\n",
    "new_thriller = 0.0\n",
    "new_rating_count = 3\n",
    "new_war = 4\n",
    "new_western = 0\n",
    "user_vec = np.array([[new_user_id, new_rating_count, new_rating_ave,\n",
    "                      new_action, new_adventure, new_animation, new_childrens,\n",
    "                      new_comedy, new_crime, new_documentary,\n",
    "                      new_drama, new_fantasy, 0.0,  # Film-Noir\n",
    "                      new_horror, 0.0,  # Musical\n",
    "                      new_mystery, new_romance, new_scifi, new_thriller, \n",
    "                      new_war, new_western, 0.0]])  # (no genres listed)\n",
    "len(item_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "6e7e557a-e5d2-41b4-a5ac-686d5b037e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m304/304\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n"
     ]
    }
   ],
   "source": [
    "def gen_user_vecs(user_vec, num_items):\n",
    "    \"\"\" given a user vector return:\n",
    "        user predict maxtrix to match the size of item_vecs \"\"\"\n",
    "    user_vecs = np.tile(user_vec, (num_items, 1))\n",
    "    return user_vecs\n",
    "\n",
    "# generate and replicate the user vector to match the number movies in the data set.\n",
    "user_vecs = gen_user_vecs(user_vec,len(item_vecs))\n",
    "\n",
    "# scale our user and item vectors\n",
    "suser_vecs = scalerUser.transform(user_vecs)\n",
    "sitem_vecs = scalerItem.transform(item_vecs)\n",
    "\n",
    "# make a prediction\n",
    "y_p = model.predict([suser_vecs[:, u_s:], sitem_vecs[:, i_s:]])\n",
    "\n",
    "# unscale y prediction \n",
    "y_pu = scalerTarget.inverse_transform(y_p)\n",
    "\n",
    "# sort the results, highest prediction first\n",
    "sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  #negate to get largest rating first\n",
    "sorted_ypu   = y_pu[sorted_index]\n",
    "sorted_items = item_vecs[sorted_index]  #using unscaled vectors for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "28f634db-1114-45fd-99a5-63de80e6b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pred_movies(y_p, item, movie_dict, maxcount=10):\n",
    "    \"\"\" print results of prediction of a new user. inputs are expected to be in\n",
    "        sorted order, unscaled. \"\"\"\n",
    "    count = 0\n",
    "    disp = [[\"y_p\", \"movie id\", \"rating ave\", \"title\", \"genres\"]]\n",
    "\n",
    "    for i in range(0, y_p.shape[0]):\n",
    "        if count == maxcount:\n",
    "            break\n",
    "        count += 1\n",
    "        movie_id = item[i, 0].astype(int)\n",
    "        disp.append([np.around(y_p[i, 0], 1), item[i, 0].astype(int), np.around(item[i, 2].astype(float), 1),\n",
    "                     movie_dict[movie_id]['title'], movie_dict[movie_id]['genres']])\n",
    "\n",
    "    table = tabulate(disp, tablefmt='html', headers=\"firstrow\")\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "733f932b-1c0d-4265-9c24-24773b263cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shlok\\AppData\\Local\\Temp\\ipykernel_56184\\284067278.py:11: RuntimeWarning: invalid value encountered in cast\n",
      "  movie_id = item[i, 0].astype(int)\n",
      "C:\\Users\\shlok\\AppData\\Local\\Temp\\ipykernel_56184\\284067278.py:12: RuntimeWarning: invalid value encountered in cast\n",
      "  disp.append([np.around(y_p[i, 0], 1), item[i, 0].astype(int), np.around(item[i, 2].astype(float), 1),\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[285], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtabulate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tabulate\n\u001b[1;32m----> 2\u001b[0m \u001b[43mprint_pred_movies\u001b[49m\u001b[43m(\u001b[49m\u001b[43msorted_ypu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msorted_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmovie_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxcount\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[284], line 13\u001b[0m, in \u001b[0;36mprint_pred_movies\u001b[1;34m(y_p, item, movie_dict, maxcount)\u001b[0m\n\u001b[0;32m     10\u001b[0m     count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     11\u001b[0m     movie_id \u001b[38;5;241m=\u001b[39m item[i, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     12\u001b[0m     disp\u001b[38;5;241m.\u001b[39mappend([np\u001b[38;5;241m.\u001b[39maround(y_p[i, \u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m), item[i, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m), np\u001b[38;5;241m.\u001b[39maround(item[i, \u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m), \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m---> 13\u001b[0m                  \u001b[43mmovie_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmovie_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, movie_dict[movie_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenres\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[0;32m     15\u001b[0m table \u001b[38;5;241m=\u001b[39m tabulate(disp, tablefmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m'\u001b[39m, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirstrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m table\n",
      "\u001b[1;31mKeyError\u001b[0m: 'title'"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print_pred_movies(sorted_ypu, sorted_items, movie_dict, maxcount = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "7daae46c-4dfa-479c-aebd-93c8c936e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_cb, y_cb, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "899b2bd3-115d-4815-8f67-21562c6c9203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover validation set (userId, movieId) pairs\n",
    "val_user_ids = cb_dataset.loc[X_val.index, 'userId'].values\n",
    "val_movie_ids = cb_dataset.loc[X_val.index, 'movieId'].values\n",
    "\n",
    "# Map userId and movieId to matrix indices\n",
    "unique_users = ratings_df['userId'].unique()\n",
    "unique_movies = ratings_df['movieId'].unique()\n",
    "\n",
    "user_id_to_idx = {uid: idx for idx, uid in enumerate(unique_users)}\n",
    "movie_id_to_idx = {mid: idx for idx, mid in enumerate(unique_movies)}\n",
    "\n",
    "# Build list of predicted values from CF model\n",
    "cf_preds_aligned = np.array([\n",
    "    cf_preds_full[movie_id_to_idx[mid], user_id_to_idx[uid]]\n",
    "    for uid, mid in zip(val_user_ids, val_movie_ids)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "ed6eb854-3148-41b8-86d4-08f54cc26265",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer \"functional_18\" expects 2 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(32,) dtype=float32>, <tf.Tensor 'data_1:0' shape=(32, 21) dtype=float32>, <tf.Tensor 'data_2:0' shape=(32, 21) dtype=float32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[288], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m val_item_scaled \u001b[38;5;241m=\u001b[39m scalerItem\u001b[38;5;241m.\u001b[39mtransform(val_item_vecs)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m cb_preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_user_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_user_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_item_scaled\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m cb_preds \u001b[38;5;241m=\u001b[39m scalerTarget\u001b[38;5;241m.\u001b[39minverse_transform(cb_preds)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Blend predictions and compute RMSE\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Application\\Anaconda\\envs\\tf310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\Application\\Anaconda\\envs\\tf310\\lib\\site-packages\\keras\\src\\layers\\input_spec.py:160\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mflatten(inputs)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(input_spec):\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expects \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(input_spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input(s),\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but it received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(inputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m input tensors. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInputs received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m     )\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_index, (x, spec) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(inputs, input_spec)):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Layer \"functional_18\" expects 2 input(s), but it received 3 input tensors. Inputs received: [<tf.Tensor 'data:0' shape=(32,) dtype=float32>, <tf.Tensor 'data_1:0' shape=(32, 21) dtype=float32>, <tf.Tensor 'data_2:0' shape=(32, 21) dtype=float32>]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Extract the validation set user/movie IDs\n",
    "val_user_ids = cb_dataset.loc[X_val.index, 'userId'].values\n",
    "val_movie_ids = cb_dataset.loc[X_val.index, 'movieId'].values\n",
    "\n",
    "# Map userId and movieId to array indices\n",
    "val_user_vecs = np.array([user_train_unscaled[user_id_to_idx[uid]] for uid in val_user_ids])\n",
    "val_item_vecs = np.array([item_train_unscaled[movie_id_to_idx[mid]] for mid in val_movie_ids])\n",
    "\n",
    "# Scale the vectors\n",
    "val_user_scaled = scalerUser.transform(val_user_vecs)\n",
    "val_item_scaled = scalerItem.transform(val_item_vecs)\n",
    "\n",
    "# Make predictions\n",
    "cb_preds = model.predict([val_user_scaled[:, 0], val_user_scaled[:, 1:], val_item_scaled[:, :-1]])\n",
    "cb_preds = scalerTarget.inverse_transform(cb_preds)\n",
    "\n",
    "# Blend predictions and compute RMSE\n",
    "for alpha in np.linspace(0, 1, 11):  # 0.0 to 1.0 in steps of 0.1\n",
    "    final_preds = alpha * cf_preds_aligned + (1 - alpha) * cb_preds\n",
    "    rmse = mean_squared_error(y_val, final_preds, squared=False)\n",
    "    print(f\"α = {alpha:.1f} | RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9ec6b-2df9-43cd-a3a3-953a6682936a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae33c95-5b51-45cd-8f26-2fd766fda40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7103ea-eb8a-4b98-bc75-2f288ec97824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
